# 1、数学基础

   不需要特意去弥补一些数学知识。题主在题目中也说了自己特意的学习了微积分，线性代数，但是然并卵。
并不是因为你学习这些知识本身没有收获，而是没有针对性。我的建议是，碰到看不懂知识点的时候再去找资料专门学习这一块，
这样是因为一方面效果很明显，能增强信心，另一方面也避免了漫无目的地毯式搜索的盲目性。
机器学习里面涉及到的数学知识大部分在本科教材中，如微积分，线代，概率论都有学过。
就单个知识点来看，并不是很高深的东西，但是如果把它们放在一起去使用，就不是那么好看了，可能还是对知识点的熟悉程度不够。
另外就是，凡事本科大学老师上课略去不讲的知识点（即所谓的非考试重点），都是以后要经常用到的。
这恐怕也是题主觉得看那些数学知识比较头大的一个重要因素。  
——摘录自 [机器学习应该准备哪些数学预备知识？|知乎问答](https://www.zhihu.com/question/36324957/answer/67094190)  

### 1.1 博文
[维基百科 | 最小二乘法](https://zh.wikipedia.org/zh-cn/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95)  
[机器学习中的数学(3)-模型组合(Model Combining)之Boosting与Gradient Boosting]
(http://www.cnblogs.com/LeftNotEasy/archive/2011/01/02/machine-learning-boosting-and-gradient-boosting.html)  
[机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)]
(http://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html)   
[机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用]
(http://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html)  
[Things of Math | HujiaweiBujidao的数学笔记](http://hujiaweibujidao.github.io/blog/2015/12/30/Things-of-Math/)  

### 1.2 公开课程
* 张志华老师在交大ieee班和acm班机器学习课程  
    1. [机器学习导论](http://ocw.sjtu.edu.cn/G2S/OCW/cn/CourseDetails.htm?Id=397)  
    2. [统计机器学习](http://ocw.sjtu.edu.cn/G2S/OCW/cn/CourseDetails.htm?Id=398)  

* 麻省理工公开课：线性代数  
网易公开课地址：<http://open.163.com/special/opencourse/daishu.html>  
麻省理工公开课：线性代数作者&讲课者：麻省理工的一个非常有趣幽默，人格魅力爆棚&
解析集聚洞察力直指根本，把国内大部分线性代数教材轰的渣也不剩的老教授Gilbert Strang。

* Harvard CS 109 Data Science的课程  
<http://cs109.github.io/2015/>

* Stanford CS229 Machine Learning
<http://cs229.stanford.edu/>


# 2、机器学习
### 2.1 总结性资料 
* [Data-Science-45min-Intros](https://github.com/DrSkippy/Data-Science-45min-Intros)
* [机器学习与模式识别 | 简书](http://www.jianshu.com/collection/1395428608b4)  
* [8步从Python白板到专家，从基础到深度学习](http://dataunion.org/15057.html)  
* [机器学习经典算法详解及Python实现 | Adan](http://blog.csdn.net/suipingsp/article/category/2749113)   
* [机器学习算法系列 | null的专栏](http://blog.csdn.net/google19890102/article/details/39781573)  

********* 

* [机器学习入门资源不完全汇总](http://ml.memect.com/article/machine-learning-guide.html)
* [tornadomeet资料目录](http://www.cnblogs.com/tornadomeet/archive/2012/05/24/2515980.html)  
* [mastering-machine-learning-scikit-learn中文版](https://muxuezi.github.io/posts/0-perface.html) 
* [机器学习资料大汇总](http://www.kuqin.com/shuoit/20150923/348242.html)  
* [机器学习常见算法分类汇总](http://blog.jobbole.com/77620/)  
* [机器学习算法基础概念学习总结](http://blog.jobbole.com/74716/)  
* [机器学习算法基础知识](http://www.36dsj.com/archives/8911)  
* [数据挖掘18大算法实现以及其他相关经典DM算法](https://github.com/linyiqun/DataMiningAlgorithm)

### 2.2 其他杂项
* [Ultimate guide for Data Exploration in Python using NumPy, Matplotlib and Pandas]
(http://www.analyticsvidhya.com/blog/2015/04/comprehensive-guide-data-exploration-sas-using-python-numpy-scipy-matplotlib-pandas/)
* [R语言学习笔记 ](http://blog.csdn.net/jack237/article/details/8210598)
* [10 种机器学习算法的要点（附 Python 和 R 代码）](http://blog.jobbole.com/92021/)  
* [网上又一位牛人的Machine Learning实验笔记]
(http://blog.csdn.net/denghp83/article/details/8996662#0-tsina-1-56037-397232819ff9a47a7b7e80a40613cfe1)
* [Bin的专栏  Machine Learning](http://www.cnblogs.com/xbinworld/tag/Machine%20Learning/)

### 2.3 数据降维
PCA
SVD


# 3、特征工程 
* 《Selecting good features》特征工程系列文章  
    1. [Part1.单变量选取](http://blog.datadive.net/selecting-good-features-part-i-univariate-selection/)  
    2. [Part2.线性模型和正则化](http://blog.datadive.net/selecting-good-features-part-ii-linear-models-and-regularization/)   
    3. [Part3.随机森林](http://blog.datadive.net/selecting-good-features-part-iii-random-forests/)    
    4. [Part4.稳定性选择法、递归特征排除法(RFE)及综合比较](http://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side/)     
* [Learning from the best](http://blog.kaggle.com/2014/08/01/learning-from-the-best/)  
* [特征工程的方方面面,Discover Feature Engineering, How to Engineer Features and How to Get Good at It]
  (http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)  
* [8 Proven Ways for improving the “Accuracy” of a Machine Learning Model]
(http://www.analyticsvidhya.com/blog/2015/12/improve-machine-learning-results/)  
* [8个经过证实的方法：提高机器学习模型的准确率](http://www.jianshu.com/p/2a7e201d28c2)  
* [Feature Engineering: How to transform variables and create new ones?]
(http://www.analyticsvidhya.com/blog/2015/03/feature-engineering-variable-transformation-creation/)   
* [干货：结合Scikit-learn介绍几种常用的特征选择方法](http://dataunion.org/14072.html)  
* [用于机器学习特征工程的哈希](http://www.diku.dk/summer-school-2014/course-material/john-langford/hashing_copenhage_2014.pdf)
* [【特征工程】特征选择与特征学习](http://www.jianshu.com/p/ab697790090f)  
* [特征工程](http://www.jianshu.com/p/b2cc338ce9e5)  
* [机器学习中的数据清洗与特征处理综述](http://tech.meituan.com/machinelearning-data-feature-process.html)  
* [A Comprehensive guide to Data Exploration](http://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/)
* [Techniques to improve the accuracy of your Predictive Models](http://pan.baidu.com/s/1mgKPzo0)
* [Techniques to improve the accuracy of your Predictive Models.pdf](http://pan.baidu.com/s/1eQCokPs)
* [introduction-ensemble-learning](http://www.analyticsvidhya.com/blog/2015/08/introduction-ensemble-learning/)  

# 4、集成学习(ensemble learning)算法
[【机器学习基础】混合和装袋](http://www.jianshu.com/p/037e1bad9fa8)  
## Bagging
基于数据随机重抽样的分类器构建方法

   * 随机森林（Random Forest）[【机器学习基础】随机森林算法](http://www.jianshu.com/p/667c9ea6cf96)

## Boosting

   * 自适应提升树 AdaBoost  
     [【机器学习基础】自适应提升](http://www.jianshu.com/p/2d5db988662a)   
     [【机器学习基础】梯度提升决策树](http://www.jianshu.com/p/819a21e1e8ef)  

# 5、技术博客&网站
* [算法组](http://suanfazu.com/)  
* [火光摇曳](http://www.flickering.cn/)  

# 6、数据挖掘比赛
* [kaggle](https://www.kaggle.com/)
* [Analytics Vidhya](http://datahack.analyticsvidhya.com/)
* [Data Castle](http://pkbigdata.com/)

# 7、其他
* [常见面试之机器学习算法思想简单梳理](http://www.cnblogs.com/tornadomeet/p/3395593.html)
* [互联网公司机器学习职位面试考察点](https://www.zhihu.com/question/25565713/answer/31981961)
* [码农翻墙去美帝——数学和机器学习准备](http://blog.sina.com.cn/s/blog_687bf5050102vawn.html)
* [公司内部推荐系统架构图](https://www.processon.com/view/link/552e1789e4b0193bc0401a86)
